{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe18f8e-3cff-409c-9c30-58f30aa024a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG system... This may take a minute.\n",
      "Processed MDD: 79 chunks.\n",
      "Processed MDR: 282 chunks.\n",
      "RAG system initialized successfully in 16.45 seconds.\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://9c7145e07da52c5374.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9c7145e07da52c5374.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import chromadb\n",
    "import uuid\n",
    "from pypdf import PdfReader\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure the generative AI model\n",
    "load_dotenv() #loads the API key put in a .env file\n",
    "try:\n",
    "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Google AI. Please ensure your API key is correct. Error: {e}\")\n",
    "\n",
    "# Define file paths for your documents\n",
    "current_dir = os.getcwd() \n",
    "FILE_PATH_MDD = os.path.join(current_dir, '..', 'documents', 'MDD.pdf')\n",
    "FILE_PATH_MDR = os.path.join(current_dir, '..', 'documents', 'MDR.pdf')\n",
    "COLLECTION_NAME = \"mdr_gap_analysis_app\"\n",
    "\n",
    "# Data processing function. Uses a smart logic to skip short paragraph \n",
    "# while maintaining titles and proximity between titles and the related paragraph.\n",
    "def process_pdf_with_smart_chunking(reader, source_name):\n",
    "    all_text_chunks = []\n",
    "    all_metadatas = []\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            continue\n",
    "        lines = text.split('\\n')\n",
    "        current_chunk = \"\"\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            is_potential_title = ( # checks if the paragraph looks like a title\n",
    "                len(stripped_line) > 5 and len(stripped_line) < 80 and\n",
    "                stripped_line.isupper() and not stripped_line.endswith('.')\n",
    "            )\n",
    "            if is_potential_title: # if it is, saves the current chunk and starts a new one starting with the title\n",
    "                if current_chunk:\n",
    "                    all_text_chunks.append(current_chunk.strip())\n",
    "                    all_metadatas.append({'source': source_name, 'page': page_num + 1})\n",
    "                current_chunk = stripped_line + \"\\n\"\n",
    "            else:\n",
    "                current_chunk += line + \" \"\n",
    "        if current_chunk:\n",
    "            all_text_chunks.append(current_chunk.strip())\n",
    "            all_metadatas.append({'source': source_name, 'page': page_num + 1})\n",
    "    return all_text_chunks, all_metadatas\n",
    "\n",
    "# Initialize the RAG System (Vector Database with ChromaDB)\n",
    "\n",
    "print(\"Initializing RAG system... This may take a minute.\")\n",
    "start_time = time.time()\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Define the local embedding function (Chroma actually have its own default, but Sentence Transformer seems good)\n",
    "from chromadb.utils import embedding_functions\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete the collection if it already exists to ensure a fresh start and create the new one\n",
    "if COLLECTION_NAME in [c.name for c in client.list_collections()]:\n",
    "    client.delete_collection(name=COLLECTION_NAME)\n",
    "    print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Process and load both documents into the database\n",
    "try:\n",
    "    reader_mdd = PdfReader(FILE_PATH_MDD)\n",
    "    chunks_mdd, metas_mdd = process_pdf_with_smart_chunking(reader_mdd, 'MDD 93/42/EEC')\n",
    "    print(f\"Processed MDD: {len(chunks_mdd)} chunks.\")\n",
    "\n",
    "    reader_mdr = PdfReader(FILE_PATH_MDR)\n",
    "    chunks_mdr, metas_mdr = process_pdf_with_smart_chunking(reader_mdr, 'MDR 2017/745')\n",
    "    print(f\"Processed MDR: {len(chunks_mdr)} chunks.\")\n",
    "\n",
    "    # Combine and add to the collection\n",
    "    text_chunks = chunks_mdd + chunks_mdr\n",
    "    metadatas = metas_mdd + metas_mdr\n",
    "    collection.add(\n",
    "        ids=[str(uuid.uuid4()) for _ in text_chunks],\n",
    "        documents=text_chunks,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"RAG system initialized successfully in {end_time - start_time:.2f} seconds.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nERROR: One of the PDF files was not found. Please check the file paths.\")\n",
    "    collection = None # Disable the app if files are not found\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during initialization: {e}\")\n",
    "    collection = None\n",
    "\n",
    "# Core Function for Querying and Generation\n",
    "\n",
    "def get_gap_analysis(question): # The actual function called by Gradio at runtime\n",
    "    if not question or not question.strip():\n",
    "        return \"Please enter a question before submitting.\"\n",
    "    if not collection:\n",
    "        return \"Error: The RAG system is not initialized. Please check the file paths and restart the notebook.\"\n",
    "    \n",
    "    print(f\"\\nReceived question: {question}\")\n",
    "    \n",
    "    # Query the vector store to get relevant context from both documents\n",
    "    results = collection.query(query_texts=[question], include = [\"documents\", \"metadatas\", \"distances\"], n_results=10)\n",
    "\n",
    "    sources_markdown = \"### Sources Used for Analysis\\n\\n\"\n",
    "    retrieved_documents = results['documents'][0]\n",
    "    retrieved_metadatas = results['metadatas'][0]\n",
    "    retrieved_distances = results['distances'][0]\n",
    "\n",
    "    for i, (doc, meta, dist) in enumerate(zip(retrieved_documents, retrieved_metadatas, retrieved_distances)):\n",
    "        # Convert distance to a more intuitive similarity score (1 - distance)\n",
    "        relevance_score = 1 - dist\n",
    "        source_info = f\"**Source {i+1}:** {meta.get('source', 'N/A')}, Page {meta.get('page', 'N/A')}\\n\"\n",
    "        relevance_info = f\"**Relevance Score:** {relevance_score:.2f}\\n\\n\"\n",
    "        content_info = f\"```\\n{doc}\\n```\\n\\n---\\n\\n\"\n",
    "        sources_markdown += source_info + relevance_info + content_info\n",
    "            \n",
    "    # Separate the context by source\n",
    "    context_mdr = \"\"\n",
    "    context_mdd = \"\"\n",
    "    for doc, meta, distance in zip(retrieved_documents, retrieved_metadatas, retrieved_distances):\n",
    "        if meta.get('source') == 'MDR 2017/745':\n",
    "            context_mdr += f\"[Page {meta.get('page', 'N/A')}]: {doc}, distance: {distance}\\n\\n\"\n",
    "        elif meta.get('source') == 'MDD 93/42/EEC':\n",
    "            context_mdd += f\"[Page {meta.get('page', 'N/A')}]: {doc}, distance: {distance}\\n\\n\"\n",
    "\n",
    "    # Engineer the prompt for the LLM (mid complexity prompt)\n",
    "    prompt = f\"\"\"\n",
    "    Act as a Senior Regulatory Consultant for a MedTech company. Your task is to perform a gap analysis based on the user's question, using **exclusively** the provided context from two documents: the old Medical Device Directive (MDD) and the new Medical Device Regulation (MDR).\n",
    "\n",
    "    Follow this structure for your response:\n",
    "    1.  **Summary of MDD Requirements:** Based on the MDD context, briefly summarize the old requirements.\n",
    "    2.  **Summary of MDR Requirements:** Based on the MDR context, summarize the new, more demanding requirements.\n",
    "    3.  **Gap Analysis:** Clearly highlight the key differences and new obligations the company needs to address.\n",
    "    4.  **Strategic Recommendation:** Provide a concise, actionable recommendation.\n",
    "\n",
    "    **Crucial Rule:** If the context for one of the documents is missing or insufficient, state that clearly. Do not invent information.\n",
    "\n",
    "    ---\n",
    "    **CONTEXT FROM MDD 93/42/EEC:**\n",
    "    {context_mdd if context_mdd else \"No specific context retrieved.\"}\n",
    "\n",
    "    ---\n",
    "    **CONTEXT FROM MDR 2017/745:**\n",
    "    {context_mdr if context_mdr else \"No specific context retrieved.\"}\n",
    "\n",
    "    ---\n",
    "    **USER QUESTION:** {question}\n",
    "\n",
    "    **CONSULTANT'S ANALYSIS:**\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM to generate the analysis\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        response = model.generate_content(prompt)\n",
    "        print(\"Successfully generated response from LLM.\")\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling the LLM: {e}\")\n",
    "        return f\"An error occurred while generating the response: {e}\"\n",
    "\n",
    "# Create and Launch the Gradio App\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# MDR Gap Analysis AI Companion\")\n",
    "    gr.Markdown(\"### Welcome.\\nPose your question below to analyze the regulatory differences between the MDD and MDR.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How have the requirements for post-market surveillance changed from the MDD to the MDR?\",\n",
    "            lines=3\n",
    "        )\n",
    "    \n",
    "    submit_button = gr.Button(\"Get Analysis!\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        answer_output = gr.Markdown(label=\"Consultant's Analysis\")\n",
    "        \n",
    "    # Define the examples to show in the UI\n",
    "    examples = [\n",
    "        \"How have the requirements for post-market surveillance changed from the MDD to the MDR?\",\n",
    "        \"What are the new requirements for the Unique Device Identification (UDI) system under the MDR, and how does this compare to the MDD?\",\n",
    "        \"What are the most significant new clinical evaluation requirements for a Class IIa device to be compliant with the MDR?\"\n",
    "    ]\n",
    "    gr.Examples(examples=examples, inputs=question_input)\n",
    "\n",
    "    # Link the button to the function\n",
    "    submit_button.click(\n",
    "        fn=get_gap_analysis,\n",
    "        inputs=question_input,\n",
    "        outputs=answer_output\n",
    "    )\n",
    "\n",
    "# Launch the app. In a Jupyter Notebook, the interface will appear directly in the cell output.\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601648ce-f6ea-4e36-9866-099be6504339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
