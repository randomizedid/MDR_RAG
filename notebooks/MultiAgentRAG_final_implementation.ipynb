{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d4de6b-4706-4a4e-b3b4-0cac0b182c12",
   "metadata": {},
   "source": [
    "# Brief Paper explaining design choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86a40c-00bd-4dfa-bd28-67dce3e720b1",
   "metadata": {},
   "source": [
    "# Multi-Agent RAG system\n",
    "This Jupyter Notebook serves as a proof of concept of a multi-agent solution for MedTech regulations. In order, you'll find a brief paper explaining design choices, the README to run the code, and the code itself divided into cells.\n",
    "\n",
    "The technical intent of the system is to provide clear answers to questions on WHO and FDA documentations on medical devices, using a 3-to-4 agents system composed like this:\n",
    "\n",
    "- an LLM orchestrator that receives the question and coordinates agents;\n",
    "- a RAG agent capable of retrieving documents related to the question;\n",
    "- an LLM response agent to put together the answer based on the documents retrieved and the question;\n",
    "- a possible fourth agent to be decided (chunking agent, source-verifier agent, compare agent, prompt agent to improve prompts, etc.).\n",
    "\n",
    "## Design Choices\n",
    "\n",
    "The major constraints were time and no use of AI or other multi-agentic solutions, therefore I defaulted to a working prototype of a multi-agent RAG using standards methods and libraries.\n",
    "\n",
    "### 1. RAG Implementation\n",
    "\n",
    "I am familiar with ChromaDB, therefore I went with it. \n",
    "\n",
    "#### 1.1 Chunking Logic\n",
    "\n",
    "Initially, I built the simplest possible RAG, with a chunking logic based on a fixed character length. Then I had a few different options:\n",
    "\n",
    "- Paragraph logic: divide documents into paragraph and ignore short ones. **In the end I used this one as it is not the optimal solution but is the best one I was able to implement in the time I had**;\n",
    "- Text conversion: converted PDFs into txt and tried a few different ideas. Unexpectedly, txt conversion worked worse than PDF;\n",
    "- Semantic chunking: find a way to create logically coherent paragraph using embeddings and similarity scores. I didn't have the time to implement this logic from scratch;\n",
    "- Agentic chunking: create a new AI agent to handle this task. I tried it and it worked pretty well but I got stuck in parsing and ran out of time.\n",
    "\n",
    "#### 1.2 Embedding Function\n",
    "\n",
    "I decided to use the deafult Chroma library (all-MiniLM-L6-v2) for ease of use, cost and efficiency. In production I probably would evaluate the trade offs of the openai embedding function.\n",
    "\n",
    "#### 1.3 Other Considerations\n",
    "\n",
    "I decided not to go with multi-hop retrieval, as it was too complicated for the simple chunking logic I had. I think that with semantic/agentic chunking, a multi-hop retrieval with (possibly) a retrieval agent could give good results.\n",
    "\n",
    "### 2. Prompting\n",
    "\n",
    "#### 2.1 Prompting Structure\n",
    "\n",
    "In the prompt I followed this structure:\n",
    "\n",
    "- Clear role\n",
    "- Explicit tasks\n",
    "- Output formatting\n",
    "- Contextual information\n",
    "- Additional Guidelines and edge cases\n",
    "\n",
    "The response agent also got a persona, since it is the agent that communicates with the users. For this particular case, I decided not to use any fancy technique (chain of thought, tree of thought, self-consistency, etc.), but to provide (when useful) one example (so one-shot prompting).\n",
    "\n",
    "#### 2.2 Output Parameters and Structure\n",
    "\n",
    "I used the built-in generation-config to set the parameters for the LLM response. I set the temperature to 0.2 (low since I want it to be repeatable and reliable), top k and p to the lower end of the standard range, and the max output tokens to 8192 (actually an unimportant parameter for 1.5-flash, since it is already concise enough).\n",
    "\n",
    "The output of the orchestrator needs to be standardized in order to be fed to the agents. I chose a JSON format like {\"agent_to_call\":\"\", \"output\": \"\", \"relevant_info\":\"\"}, and it worked pretty well. I also tried to use the outputschema from generation_config but it worked worse. I think it probably is a better option but needs more fine-tuning (like Pydantic).\n",
    "\n",
    "### 3. Agent Orchestration\n",
    "\n",
    "Since I have a premium subscription, I chose Gemini as LLM. Among the different models, the ones who showed better behaviour were 1.5-flash and 1.5 pro. In the end I opted for 1.5-flash as it increases speed and efficiency without losing much precision.\n",
    "\n",
    "#### 3.1 Handling Data Flow between Agents\n",
    "\n",
    "In a multi-agent system, it is crucial to standardize the inputs and outputs of the agents, since you'll wrap things in a loop that will execute tasks and you won't be able to scale things if you have custom I/O for your agents. I chose to apply a brute-force workflow, where every agent has its own memory and it gets shared throughout the execution of tasks, serving as a context for both the orchestrator and the other agents. With more time, I think the best option (always assuming no pre-made agentic solutions) would be to create a 'state' object that stores information and that agents can access to gather the info they need and update it before passing the control to the next one. Additionally, an issue with this type of memory is that all agents have access to everything that is happened. It would be better to also create levels of permission, it could be useful to hide knowledge from agents at certain steps.\n",
    "\n",
    "I decided to go with 3 different classes as the 3 agents in this case are very different: one is the orchestrator, one is a non-intelligent agent and the third is execution agent that can be generalized to other agents. In fact, the code for the chunking agent I left in is extremely similar to the response agent (in production I would make a single class for every intelligent agent).\n",
    "\n",
    "#### 3.2 Pros and Cons\n",
    "\n",
    "- The main pro of having an orchestrator is that the system can be easily evolved and scaled. You can add as many agents as you want and let the orchestrator decide whether and when to call the, without having to hard code a workflow;\n",
    "- Another pro is the flexibility of the workflow, different tasks will result in different workflow, and with an ideal system you can tailor the complexity of the execution to the need of the query, boosting efficiency.\n",
    "\n",
    "- A con of having an orchestrator is the introduction of non-determinism in the execution workflow. You can be 99% sure that everything will work out well, but you never know whether your system will try to call the response agent before the document retrieval, for example. You can counter it by adding more control to the prompt, but this allows for less reasoning and freedom for the model's intelligence, and adds complexity to the design.\n",
    "- Another con is that for simple tasks like this one, the orchestrator adds useless complexity, response time and costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d453f8-4328-4514-8669-aacdcc253567",
   "metadata": {},
   "source": [
    "# That was the brief paper, now there's the README file I would publish if this was a single repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5df5f-2628-4897-ae0d-f2f309ec24a7",
   "metadata": {},
   "source": [
    "# MedTech Regulatory AI Companion: A RAG-based Tool\n",
    "\n",
    "This code contains the development of a multi-agent AI-powered assistant to navigate the complex landscape of Medical Device regulations. The primary goal is to showcase how Retrieval-Augmented Generation (RAG) can be used to perform a document analysis on FDA and WHO medical de\n",
    "\n",
    "This project was developed as a proof-of-concept to address the significant regulatory challenges faced by MedTech companies.\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "In this code you will find a 4-agent approach to document analysis:\n",
    "\n",
    "* An orchestrator agent\n",
    "* A chunking agent for semantic chunking\n",
    "* A non-intelligent RAG agent for information retrieval\n",
    "* A response agent to tailor the answer\n",
    "\n",
    "---\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "* **Language:** Python\n",
    "* **Core AI/RAG Frameworks:** ChromaDB (Vector Store), Sentence-Transformers (Embeddings)\n",
    "* **LLM Integration:** Google Gemini (`google-generativeai`)\n",
    "* **PDF Processing:** `pymupdf`\n",
    "* **Environment Management:** Conda, Pip, `python-dotenv`\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Follow these steps to set up and run the project locally.\n",
    "\n",
    "### 1. Prerequisites\n",
    "\n",
    "* Python 3.9+\n",
    "* Conda or another virtual environment manager\n",
    "\n",
    "### 2. Installation & Setup\n",
    "\n",
    "1.  **Download the code and documents** \n",
    "\n",
    "2.  **Create and activate a Conda environment:**\n",
    "    ```bash\n",
    "    conda create -n mdrag_env python=3.9\n",
    "    conda activate mdrag_env \n",
    "    ``` \n",
    "\n",
    "3.  **Install libraries:**\n",
    "    ```bash\n",
    "    pip install [any library you see imported, I will soon create a requirements.txt file]\n",
    "    ``` \n",
    "\n",
    "4.  **Set up your API Key:**\n",
    "    * Create a file named `.env` in the root directory of the project.\n",
    "    * Add your Google Gemini API key to this file:\n",
    "        ```\n",
    "        GOOGLE_API_KEY=\"AIzaSy...your-key-here\"\n",
    "        ```\n",
    "\n",
    "5.  **Add the Documents:**\n",
    "    * Create a folder named `documents` in the root directory.\n",
    "    * Place your files inside this folder.\n",
    "\n",
    "6.  **Run the code:**\n",
    "    * Run the code cell by cell. It should work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40713cc2-97db-4577-a884-fd2fb1f081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import chromadb\n",
    "import uuid\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pymupdf4llm\n",
    "import json\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30825fc5-b1f5-43de-a6e7-c9f0ce7f3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environmental variables and AI models\n",
    "\n",
    "load_dotenv() # load the API key and put in a .env file\n",
    "try:\n",
    "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Google AI. Please ensure your API key is correct. Error: {e}\")\n",
    "\n",
    "# Configure paths to data\n",
    "current_dir = os.getcwd() \n",
    "FILE_PATH_FDA_DESIGN = os.path.join(current_dir, '..', 'documents', 'FDA_Design_Control_Guidance.pdf')\n",
    "FILE_PATH_WHO = os.path.join(current_dir, '..', 'documents', 'WHO_Medical_Device_Regulations.pdf')\n",
    "FILE_PATH_FDA_POLICY = os.path.join(current_dir, '..', 'documents', 'FDA_Policy_Device_Software_Functions.pdf')\n",
    "COLLECTION_NAME = \"multi_agent_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6997c9-03e2-4a41-8f6a-327a00857e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657 1657\n"
     ]
    }
   ],
   "source": [
    "# Split logic (very simple, splitting paragraphs and skipping short ones)\n",
    "\n",
    "def split_into_chunks(dict_list):\n",
    "    text_chunks = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for document in dict_list:\n",
    "        try:\n",
    "            paragraphs = list(document.values())[0].split(\"\\n\\n\")\n",
    "    \n",
    "            for paragraph in paragraphs: # Delete short paragraphs\n",
    "                if len(paragraph) > 10:\n",
    "                    text_chunks.append(paragraph)\n",
    "                    metadatas.append({\"source\": list(document.keys())[0]})\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {list(document.keys())[0]}: {e}\")\n",
    "            \n",
    "    return text_chunks, metadatas\n",
    "\n",
    "# Use pymupdf4llm to convert pdf into text, adequately formatted. Using dicts to keep track of sources and add them to metadata while chunking\n",
    "fda_design_dict = {'FDA_Design_Control_Guidance.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_DESIGN)}\n",
    "who_dict = {'WHO_Medical_Device_Regulations.pdf': pymupdf4llm.to_markdown(FILE_PATH_WHO)}\n",
    "fda_policy_dict = {'FDA_Policy_Device_Software_Functions.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_POLICY)}\n",
    "\n",
    "text_chunks, metadatas = split_into_chunks([fda_design_dict, who_dict, fda_policy_dict])\n",
    "print(len(text_chunks), len(metadatas))\n",
    "\n",
    "\"\"\"# Second chunking logic: texting up the documents. I'll convert the pdf in txt and see if the RAG handles them better.\n",
    "# Result: it worked worse.\n",
    "\n",
    "# Configure paths to data\n",
    "current_dir = os.getcwd() \n",
    "FILE_PATH_FDA_DESIGN = os.path.join(current_dir, '..', 'documents', 'FDA_Design_Control_Guidance.txt')\n",
    "FILE_PATH_WHO = os.path.join(current_dir, '..', 'documents', 'WHO_Medical_Device_Regulations.txt')\n",
    "FILE_PATH_FDA_POLICY = os.path.join(current_dir, '..', 'documents', 'FDA_Policy_Device_Software_Functions.txt')\n",
    "COLLECTION_NAME = \"multi_agent_rag\"\n",
    "\n",
    "def process_txt_file(file_path, source_name):\n",
    "    all_text_chunks = []\n",
    "    all_metadatas = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        # Chunking strategy: split by paragraph\n",
    "        paragraphs = full_text.split('\\n\\n')\n",
    "\n",
    "        for para in paragraphs:\n",
    "            stripped_para = para.strip()\n",
    "            if len(stripped_para) > 25:  # Filter out very short paragraphs or empty lines\n",
    "                all_text_chunks.append(stripped_para)\n",
    "                # Metadata is simpler for a txt file, just the source\n",
    "                all_metadatas.append({'source': source_name})\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "    return all_text_chunks, all_metadatas\n",
    "\n",
    "chunks_fdad, metas_fdad = process_txt_file(FILE_PATH_FDA_DESIGN, 'FDA_Design_Control_Guidance')\n",
    "print(f\"Processed FDA_Design: {len(chunks_fdad)} chunks.\")\n",
    "\n",
    "chunks_who, metas_who = process_txt_file(FILE_PATH_WHO, 'WHO_Medical_Device_Regulations')\n",
    "print(f\"Processed WHO: {len(chunks_who)} chunks.\")\n",
    "\n",
    "chunks_fdap, metas_fdap = process_txt_file(FILE_PATH_FDA_POLICY, 'FDA_Policy_Device_Software_Functions')\n",
    "print(f\"Processed FDA Policy: {len(chunks_fdap)} chunks.\")\n",
    "\n",
    "text_chunks = chunks_fdad + chunks_who + chunks_fdapS\n",
    "metadatas = metas_fdad + metas_who + metas_fdap\"\"\"\n",
    "\n",
    "# JSON parser function used throughout the code (could become a class method of the agents)\n",
    "def clean_response(response):\n",
    "    response_text = response.text\n",
    "    start_index = response_text.find(\"{\")\n",
    "    end_index = response_text.rfind(\"}\")\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        response_cleaned = json.loads(response_text[start_index : end_index + 1])\n",
    "        return response_cleaned\n",
    "    else:\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "455b173f-006d-4f4a-8de5-e754aef4524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent classes. Every agent will have a memory variable with a memory limit, and this will be the context that gets passed along\n",
    "# It would be cleaner to build a 'state' that every agent can access and update (similar to the logic of LangGraph), but this works fine for a POC\n",
    "\n",
    "class OrchestratorAgent:\n",
    "\n",
    "    def __init__(self, agents):\n",
    "        self.agents = agents\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        #self.model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "        #self.model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')\n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0.85,\n",
    "            \"max_output_tokens\": 8192\n",
    "        }\n",
    "\n",
    "    def run(self): # The main function, where the loop is. Keeps elaborating the input and calling an agent until the user cuts.\n",
    "        print(\"Hi! I am Camille, your MedTech regulatory companion. How can I help you?\")\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        while True:\n",
    "            self.memory = self.memory[-self.memory_limit:]\n",
    "            if user_input.lower() in [\"exit\", \"bye\", \"close\"]:\n",
    "                print(\"I hope I could be of use to you, have a great day!\")\n",
    "                break  \n",
    "            orch_response = self.orchestrate(user_input)\n",
    "            if orch_response[\"agent_to_call\"] == \"No action needed\":\n",
    "                print(\"Is there anything else I can help you with?\")\n",
    "                user_input = input(\"You: \")\n",
    "            for agent in self.agents:\n",
    "                if agent.name == orch_response[\"agent_to_call\"]:\n",
    "                    print(f\"Found agent I was looking for: {agent.name}\\n\")\n",
    "                    response = agent.act(orch_response[\"output\"], orch_response[\"relevant_info\"], self.memory)\n",
    "                    self.memory.append(f\"Agent {agent.name} responded {response}\")     \n",
    "        return response\n",
    "\n",
    "    def orchestrate(self, user_input):\n",
    "        self.memory.append(f\"User: {user_input}\")\n",
    "        self.memory = self.memory[-self.memory_limit:]\n",
    "        context = \"\\n\".join(self.memory)\n",
    "        response_format = {\"agent_to_call\":\"\", \"output\": \"\", \"relevant_info\":\"\"}\n",
    "        response = self.model.generate_content(self.get_prompt(context, response_format), generation_config = self.generation_config)\n",
    "        self.memory.append(f\"Orchestrator: {response.text}\")\n",
    "        response_cleaned = clean_response(response)\n",
    "        return response_cleaned\n",
    "\n",
    "    def get_prompt(self, context, response_format):\n",
    "        prompt = f\"\"\"\n",
    "        \n",
    "        Act as an orchestrator agent for an intelligent RAG system for MedTech companies. \n",
    "        \n",
    "        Your task is to coordinate agents in order to extract relevant documents from a RAG system and package a coherent and precise answer to the query received.\n",
    "        The task is to call a first time the ragagent, and then use the ragagent documents to call the response_agent on them and craft a response. Make sure to call the response_agent only if the memory contains a previous call to the ragagent.\n",
    "        \n",
    "        You will return instructions in a valid JSON in the form of {response_format}. All output should be of string type, the \"output\" is for the query and the \"relevant_info\" is to attach documents from the RAG for the response agent.\n",
    "        \n",
    "        Your AI agents and their descriptions are {\", \".join([f\"- {agent.name}: {agent.description}\" for agent in self.agents])}\n",
    "        Use the context, which includes the current user input and the memory of previous inputs and outputs, to plan next steps.\n",
    "        Context : {context}\n",
    "\n",
    "        Guidelines:\n",
    "        At every step, you need to choose only one of the agents and provide instruction to only that agent. If the request needs multiple agent to be solved, do that in a loop.\n",
    "        Read the context, take your time to understand the task, and check if you have executed it correctly.\n",
    "        If there are no actions needed, default the \"agent_to_call\" parameter to \"No action needed\" in the response.\n",
    "        Return only the agent name in the \"agent_to_call\" parameter.\n",
    "        \n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "class RAGAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"ragagent\"\n",
    "        self.description = \"\"\"I am a RAG agent that can search for relevant documents in a vector database in order to answer a query.\n",
    "                            I expect a user query as input and will return relevant chunks and a variable containing sources info, relevance scores and chunks.\n",
    "                            \"\"\"\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "        self.sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.collection_name = COLLECTION_NAME\n",
    "\n",
    "    def act(self, query, relevant_info, memory): # Checks whether the db exists before querying it\n",
    "        if \"RAG initialized\" not in memory:\n",
    "            self.collection = self.initialize_db()\n",
    "            self.memory.append(\"RAG initialized\")\n",
    "        self.memory.append(memory)\n",
    "        self.memory = self.memory[-self.memory_limit:]\n",
    "        documents = self.query_db(query)\n",
    "        return documents\n",
    "\n",
    "    def initialize_db(self):\n",
    "        print(\"Initializing RAG system... This may take a minute.\")\n",
    "        self.client = chromadb.Client()\n",
    "\n",
    "        if self.collection_name in [c.name for c in self.client.list_collections()]:\n",
    "            self.client.delete_collection(name = self.collection_name)\n",
    "    \n",
    "        collection = self.client.get_or_create_collection(\n",
    "        name = self.collection_name,\n",
    "        embedding_function = self.sentence_transformer_ef\n",
    "        )\n",
    "        self.load_documents(collection, text_chunks, metadatas)\n",
    "\n",
    "        return collection\n",
    "\n",
    "    def load_documents(self, collection, document_chunks, metadatas):\n",
    "        collection.add(\n",
    "        ids = [str(uuid.uuid4()) for _ in text_chunks],\n",
    "        documents = text_chunks,\n",
    "        metadatas = metadatas)\n",
    "    \n",
    "    def query_db(self, question): # Querys the db retrieving chunks, sources and embedding distances\n",
    "        results = self.collection.query(query_texts=[question], include = [\"documents\", \"metadatas\", \"distances\"], n_results=10)\n",
    "\n",
    "        sources_markdown = \"### Sources Used for Analysis\\n\\n\"\n",
    "        retrieved_documents = results['documents'][0]\n",
    "        retrieved_metadatas = results['metadatas'][0]\n",
    "        retrieved_distances = results['distances'][0]\n",
    "    \n",
    "        for i, (doc, meta, dist) in enumerate(zip(retrieved_documents, retrieved_metadatas, retrieved_distances)):\n",
    "            # Convert distance to a more intuitive relevance score (1 - distance)\n",
    "            relevance_score = 1 - dist\n",
    "            source_info = f\"**Source {i+1}:** {meta.get('source', 'N/A')}, Page {meta.get('page', 'N/A')}\\n\"\n",
    "            relevance_info = f\"**Relevance Score:** {relevance_score:.2f}\\n\\n\"\n",
    "            content_info = f\"```\\n{doc}\\n```\\n\\n---\\n\\n\"\n",
    "            sources_markdown += source_info + relevance_info + content_info\n",
    "        print(f\"Sources, documents and relevance score: {sources_markdown}\")\n",
    "            \n",
    "        return sources_markdown\n",
    "\n",
    "class ResponseAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"response_agent\"\n",
    "        self.description = \"\"\"I am a response agent that expects as input a user query and relevant documents and info from a RAG search.\n",
    "                            My task is to craft a precise response for the user based on the provided documents. I will return a response text.\n",
    "                            \"\"\"\n",
    "\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        #self.model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "        #self.model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')\n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0.85,\n",
    "            \"max_output_tokens\": 8192\n",
    "        }\n",
    "    \n",
    "    def act(self, query, relevant_info, memory): # Gets the prompt, generate the response and prints it\n",
    "        prompt = self.get_prompt(query, relevant_info, memory)\n",
    "        response = self.model.generate_content(prompt, generation_config = self.generation_config)\n",
    "        print(response.text)\n",
    "        return response.text\n",
    "\n",
    "    def get_prompt(self, query, relevant_info, memory):\n",
    "        self.memory = self.memory[-self.memory_limit:]\n",
    "        prompt = f\"\"\"\n",
    "        \n",
    "        Act as Camille, an AI companion acting as a Senior Consultant for medical devices. \n",
    "        \n",
    "        You receive a query from your client, and your task is to answer to it based on the relevant information you receive from the RAG system as document chunks.\n",
    "\n",
    "        Cite the documents and sources you receive as part of your input. The structure of your answer will be:\n",
    "        - Salutation.\n",
    "        - Precise response to the query based on the documents received from the RAG.\n",
    "        - Strategic recommendation\n",
    "\n",
    "        As additional resources and context:\n",
    "        User input: {query}\n",
    "        Relevant documents with sources and relevance scores: {relevant_info}\n",
    "        Memory of previous inputs and info: {memory}\n",
    "\n",
    "        Guidelines:\n",
    "        Be precise, confident and do not make things up. If the context is not deatiled enough to provide a clear answer, state it.\n",
    "\n",
    "        \"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205c722-486a-4c73-a07a-5f1c49ccabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added an agent to chunk documents. It works well but I couldn't end the parsing in time, therefore I didn't add it.\n",
    "# I have used the response_schema on this one, worked decently but not as good as expected.\n",
    "\n",
    "class ChunkingAgent():\n",
    "    def __init__(self):\n",
    "        self.name = \"chunking_agent\"\n",
    "        self.description = \"\"\"I am an LLM agent created for semantic chunking. My task is to receive pdf documents and return a list of semantically coherent chunks.\n",
    "                            \"\"\"\n",
    "\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "        self.text_chunks = []\n",
    "        self.metadatas = []\n",
    "        self.model = genai.GenerativeModel('gemini-2.5-pro-preview-03-25')\n",
    "        self.generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0.85,\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_json_schema\": {\"chunks\": list}\n",
    "        }\n",
    "    \n",
    "    def act(self, query, relevant_info, memory):\n",
    "        fda_design_dict = {'FDA_Design_Control_Guidance.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_DESIGN)}\n",
    "        who_dict = {'WHO_Medical_Device_Regulations.pdf': pymupdf4llm.to_markdown(FILE_PATH_WHO)}\n",
    "        fda_policy_dict = {'FDA_Policy_Device_Software_Functions.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_POLICY)}\n",
    "        documents = [fda_design_dict, who_dict, fda_policy_dict]\n",
    "\n",
    "        for document in documents:\n",
    "            source = list(document.keys())[0]\n",
    "            cleaned_chunks = self.chunk_document(list(document.values())[0])\n",
    "            self.text_chunks.append(cleaned_chunks)\n",
    "            for i in range(len(cleaned_chunks)):\n",
    "                self.metadatas.append({\"source\": source})\n",
    "        print(f\"metadatas: {self.metadatas}\")\n",
    "        return [self.text_chunks, self.metadatas]\n",
    "\n",
    "    def chunk_document(self, document):\n",
    "        cleaned_chunks = []\n",
    "        max_length = 10000\n",
    "        for i in range(0, len(document), max_length):\n",
    "            text_chunk = document[i : i + max_length]\n",
    "            prompt = self.get_prompt(text_chunk)\n",
    "            chunks = self.model.generate_content(prompt)\n",
    "            cleaned_part = clean_response(chunks)[\"chunks\"]\n",
    "            cleaned_chunks.extend(cleaned_part)\n",
    "        return cleaned_chunks\n",
    "        \n",
    "    def get_prompt(self, document):\n",
    "        #response_format = {\"chunks\": []}\n",
    "        # You MUST return a valid JSON in the form of {response_format}, where in \"chunks\" is a list where each item is a single, coherent, chunk of text from the document.\n",
    "        prompt = f\"\"\"\n",
    "        \n",
    "        Act as an expert in MedTech regulations. \n",
    "        \n",
    "        You receive a document about medical device regulations and  your task is to divide it into semantically coherent chunks for a RAG system.\n",
    "        \n",
    "        Make sure to open and close the brackets for the JSON and the list correctly. Make sure to escape possible double quotes in the text.\n",
    "        \n",
    "        Documents to chunk:\n",
    "        {document}\n",
    "\n",
    "        Guidelines:\n",
    "\n",
    "        Straightforward techniques involve separating paragraph, accounting for end of sentences indicators like '.', '!', '?' and so on.\n",
    "        Make your own techniques using multi-step reasoning (ri-validate your chunks if necessary). Avoid splitting sentences or ideas across different chunks.\n",
    "        Chunks need to be semantically coherent. Each chunk should represent a complete thought, topic, or regulatory requirement.\n",
    "        \n",
    "        \"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34d4077-9071-4fb7-b4c6-2d7c39f0bc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am Camille, your MedTech regulatory companion. How can I help you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What documentation is needed for a mobile app that  monitors heart rate?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found agent I was looking for: ragagent\n",
      "\n",
      "Initializing RAG system... This may take a minute.\n",
      "Sources, documents and relevance score: ### Sources Used for Analysis\n",
      "\n",
      "**Source 1:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.39\n",
      "\n",
      "```\n",
      "\n",
      "FDA has cleared several mobile medical apps with attachments to a mobile\n",
      "platform. Specifically, patient monitoring mobile apps that monitor a patient\n",
      "for heart rate variability from a signal produced by an electrocardiograph,\n",
      "vectorcardiograph, or blood pressure monitor are classified as cardiac\n",
      "monitoring software under 21 CFR 870.2300 (Cardiac monitor (including\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 2:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.26\n",
      "\n",
      "```\n",
      "\n",
      "cardiotachometer and rate alarm)). Other mobile medical apps that use a\n",
      "hardware attachment or interface to a monitoring system that have been\n",
      "cleared include an automatic electronic blood pressure monitor under 21 CFR\n",
      "870.1130 and a perinatal monitoring system under 21 CFR 884.2740.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 3:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.16\n",
      "\n",
      "```\n",
      "\n",
      "  - Software functions that use a sensor or electrode attached to the mobile platform or\n",
      "tools within the mobile platform itself (e.g., accelerometer) to measure physiological\n",
      "parameters during CPR and give feedback about the quality of CPR being delivered.\n",
      "Possible product code: LIX (21 CFR 870.5210).\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 4:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.15\n",
      "\n",
      "```\n",
      "\n",
      "  - Software functions that use a sensor or lead that is connected to a mobile platform to\n",
      "measure and display the electrical signal produced by the heart (electrocardiograph or\n",
      "ECG). Possible product code(s): DPS, MLC, OEY (21 CFR 870.2340), MLO, MWJ,\n",
      "DSH (21 CFR 870.2800).\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 5:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.13\n",
      "\n",
      "```\n",
      "blood glucose, blood pressure, heart rate, weight, or other data from a device to\n",
      "eventually share with a heath care professional, or upload it to an EHR that is\n",
      "certified under the ONC Health IT Certification Program, or upload it to an online\n",
      "(cloud) database, or upload it to a PHR.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 6:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.09\n",
      "\n",
      "```\n",
      "uses a mobile platform for medical device functions, such as attachment of a\n",
      "blood glucose strip reader to a mobile platform to function as a glucose meter;\n",
      "or attachment of electrocardiograph (ECG) electrodes to a mobile platform to\n",
      "measure, store, and display ECG signals; a software function that uses the\n",
      "built-in accelerometer on a mobile platform to collect motion information for\n",
      "monitoring sleep apnea; a software function that uses sensors (internal or\n",
      "external) on a mobile platform for creating electronic stethoscope function is\n",
      "considered to transform the mobile platform into an electronic stethoscope;\n",
      "manufacturers of such a mobile app are required to follow the requirements of\n",
      "21 CFR 870.1875(b) (Electronic stethoscope); and similarly, a software\n",
      "function that displays radiological images for diagnosis transforms the mobile\n",
      "platform into a Medical image management and processing system under 21\n",
      "CFR 892.2050.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 7:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.08\n",
      "\n",
      "```\n",
      "\n",
      "12. Software functions that provide historical trending and comparison of vital signs (e.g.,\n",
      "body temperature, heart rate, blood pressure, or respiratory rate).;\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 8:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.07\n",
      "\n",
      "```\n",
      "\n",
      "  - Software functions that use a sensor attached to the mobile platform or tools within\n",
      "the mobile platform itself (e.g., accelerometer, microphone) to measure physiological\n",
      "parameters (e.g., limb movement, electrical activity of the brain (EEG)) during sleep\n",
      "and are intended for use in diagnosis of specific diseases or conditions such as sleep\n",
      "apnea. Possible product code(s): OLV (21 CFR 882.1400), LEL (21 CFR 882.5050),\n",
      "BZQ, MNR, PRK (21 CFR 868.2375), FLS, NPF (21 CFR 868.2377).\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 9:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.05\n",
      "\n",
      "```\n",
      "\n",
      "For purposes of this guidance, a “mobile medical app” is a mobile app that incorporates\n",
      "device software functionality that meets the definition of a device in section 201(h) of the\n",
      "FD&C Act; [19] and either is intended:\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 10:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.02\n",
      "\n",
      "```\n",
      "For information about whether your software or mobile app is considered a medical\n",
      "device, contact [digitalhealth@fda.hhs.gov.](mailto:digitalhealth@fda.hhs.gov)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Found agent I was looking for: response_agent\n",
      "\n",
      "Dear [Client Name],\n",
      "\n",
      "Based on the provided FDA documentation (Sources 1, 2, 4, 5, 6, 7, 9), the documentation needed for a mobile app that monitors heart rate depends heavily on the app's functionality.  If the app simply displays heart rate data from a connected device (like a fitness tracker), the regulatory requirements may be less stringent than if the app analyzes the data for diagnostic purposes or uses the data to trigger alerts.\n",
      "\n",
      "Specifically, if your app:\n",
      "\n",
      "* **Displays heart rate data from a connected device:**  Sources 5 and 7 suggest that documentation regarding data sharing with healthcare professionals, EHR upload capabilities, and historical trending of heart rate data would be relevant.  However, the exact requirements are not explicitly defined in these sources.\n",
      "* **Analyzes heart rate data for diagnostic purposes or triggers alerts:** Sources 1, 2, 4, and 6 indicate that such an app would be considered a medical device under 21 CFR 870.2300 (Cardiac monitor) and would require substantial documentation to meet FDA clearance requirements.  This would likely include pre-market submissions and adherence to specific regulatory pathways.  The exact codes (DPS, MLC, OEY, MLO, MWJ, DSH) mentioned in Source 4 suggest a need for further investigation into the specific classification of your device.  Source 6 further highlights the need for compliance with regulations depending on the specific functionality (e.g., 21 CFR 870.1875(b) for electronic stethoscope functionality).\n",
      "\n",
      "**Strategic Recommendation:**\n",
      "\n",
      "Given the ambiguity surrounding the precise functionality of your heart rate monitoring app, I strongly recommend contacting the FDA directly at digitalhealth@fda.hhs.gov (Source 10) for clarification on the specific regulatory requirements and necessary documentation.  Providing a detailed description of your app's features and intended use will allow the FDA to provide tailored guidance.  This proactive approach will minimize potential delays and ensure compliance with all relevant regulations.\n",
      "\n",
      "Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Is our AI-powered MRI analysis tool considered a medical  device software?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found agent I was looking for: ragagent\n",
      "\n",
      "Initializing RAG system... This may take a minute.\n",
      "Sources, documents and relevance score: ### Sources Used for Analysis\n",
      "\n",
      "**Source 1:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.12\n",
      "\n",
      "```\n",
      "\n",
      "**Answer** : Software used in the production process for medical devices, or for collecting,\n",
      "storing and maintaining quality system data collection for medical devices (including\n",
      "complaint submissions) is not considered a medical device on its own. This software does not\n",
      "meet the definition of medical device, but is part of the quality system. However this\n",
      "software is required to comply with the appropriate good manufacturing practices (GMP)\n",
      "regulations. [94]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 2:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.12\n",
      "\n",
      "```\n",
      "\n",
      "FDA has previously clarified that when a software application is used to analyze medical\n",
      "device data, it has traditionally been regulated as an accessory to a medical device [13] or as\n",
      "medical device software. In 2014, the International Medical Device Regulators Forum\n",
      "established globally harmonized vocabulary for such software applications and defined the\n",
      "term “Software as a Medical Device (SaMD).” [14]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 3:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.10\n",
      "\n",
      "```\n",
      "\n",
      "17. **Software functions that display patient-specific medical device data –** These\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 4:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.09\n",
      "\n",
      "```\n",
      "\n",
      "**Answer** : Software used for data collection in clinical studies (such as electronic Patient\n",
      "Reported Outcomes (ePRO) apps) is not considered on its own to be a device software\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 5:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.08\n",
      "\n",
      "```\n",
      "software is not considered a device because it is not intended for use in the diagnosis\n",
      "of disease or other conditions, or in the cure, mitigation, treatment, or prevention of\n",
      "disease. Examples include software functions that:\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 6:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.04\n",
      "\n",
      "```\n",
      "\n",
      "**Answer** : FDA recognizes that this guidance does not describe all types of software used in\n",
      "health care. Some manufacturers may be unsure whether their software function is\n",
      "considered a medical device that is subject to regulatory oversight, or whether their medical\n",
      "device could be under FDA’s intent to exercise enforcement discretion. If the device is\n",
      "subject to regulatory oversight, manufacturers may have questions about which regulatory\n",
      "requirements are applicable to their software function.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 7:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.02\n",
      "\n",
      "```\n",
      "\n",
      "  - Software function that analyzes the radiologist’s reported imaging findings and other\n",
      "patient-specific medical information taken by an health care professional upon\n",
      "admission as input to a stroke triage algorithm that indicates whether to transfer the\n",
      "patient to a major stroke center for an intervention. [61]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 8:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.01\n",
      "\n",
      "```\n",
      "\n",
      "Software functions may take a number of forms, but it is important to note that FDA intends\n",
      "to apply its regulatory oversight to only the subset of software functions identified below and\n",
      "in Appendix C. These software functions can transform a general-purpose computing\n",
      "platform or mobile platform into a regulated medical device by using attachments, display\n",
      "screens, sensors, or other such methods. Regardless of the mechanism behind the\n",
      "transformation, FDA considers such software to be device software functions.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 9:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** 0.00\n",
      "\n",
      "```\n",
      "software function that analyzes patient-specific medical information to detect\n",
      "a life-threatening and/or time critical condition, such as stroke or sepsis, and\n",
      "generate an alarm or an alert to notify a health care professional; [42] software\n",
      "function that analyzes readings from a blood glucose monitor and alerts the\n",
      "user of readings outside a predetermined range; software function that\n",
      "analyzes an ECG waveform output from an FDA-cleared device to detect or\n",
      "diagnose arrhythmias (e.g., atrial fibrillation). [43] We believe that these types of\n",
      "software present the same level of risk to patients regardless of the platform\n",
      "on which they run.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Source 10:** FDA_Policy_Device_Software_Functions.pdf, Page N/A\n",
      "**Relevance Score:** -0.00\n",
      "\n",
      "```\n",
      "software functions that are solely intended to transfer, store, convert formats, and\n",
      "display medical device data or results, without controlling or altering the functions or\n",
      "parameters of any connected medical devices. These do not include software\n",
      "functions intended to generate alarms or alerts or prioritize patient-related information\n",
      "on multi-patient displays, which are typically used for active patient monitoring to\n",
      "enable immediate awareness for potential clinical intervention, and are considered\n",
      "device software functions because these functions involve analysis or interpretation of\n",
      "laboratory test or other device data and results.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Found agent I was looking for: response_agent\n",
      "\n",
      "Dear [Client Name],\n",
      "\n",
      "Based on the provided FDA documentation (Sources 1, 2, 7, 8, 9), your AI-powered MRI analysis tool is *likely* considered medical device software.  Source 2 explicitly states that software analyzing medical device data is traditionally regulated as medical device software or an accessory to a medical device.  Source 7 provides an example of software analyzing imaging findings for clinical decision-making, which is analogous to your MRI analysis tool.  Source 8 emphasizes that software transforming a general-purpose platform into a regulated medical device is considered device software.  Finally, Source 9 gives examples of software analyzing medical data to detect critical conditions, further supporting the classification of your tool as medical device software.\n",
      "\n",
      "However, the precise classification depends on the *specific functionality* of your tool. If it simply displays MRI data without interpretation or clinical decision support, the regulatory requirements might be less stringent.  If, however, it analyzes the data to provide diagnostic information or recommendations for treatment, it will almost certainly fall under the regulatory framework for medical device software, potentially as a Software as a Medical Device (SaMD) as defined in Source 2.  The provided text does not offer enough detail to definitively classify your tool.\n",
      "\n",
      "**Strategic Recommendation:**\n",
      "\n",
      "To ensure compliance and avoid potential regulatory issues, I strongly recommend a thorough review of the FDA's guidance on Software as a Medical Device (SaMD).  A detailed analysis of your tool's functionality and intended use is crucial to determine the appropriate regulatory pathway and necessary documentation.  Consulting with regulatory experts specializing in medical device software is also highly advisable.  This proactive approach will mitigate risks and ensure your product's successful market entry.\n",
      "\n",
      "Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope I could be of use to you, have a great day!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Dear [Client Name],\\n\\nBased on the provided FDA documentation (Sources 1, 2, 7, 8, 9), your AI-powered MRI analysis tool is *likely* considered medical device software.  Source 2 explicitly states that software analyzing medical device data is traditionally regulated as medical device software or an accessory to a medical device.  Source 7 provides an example of software analyzing imaging findings for clinical decision-making, which is analogous to your MRI analysis tool.  Source 8 emphasizes that software transforming a general-purpose platform into a regulated medical device is considered device software.  Finally, Source 9 gives examples of software analyzing medical data to detect critical conditions, further supporting the classification of your tool as medical device software.\\n\\nHowever, the precise classification depends on the *specific functionality* of your tool. If it simply displays MRI data without interpretation or clinical decision support, the regulatory requirements might be less stringent.  If, however, it analyzes the data to provide diagnostic information or recommendations for treatment, it will almost certainly fall under the regulatory framework for medical device software, potentially as a Software as a Medical Device (SaMD) as defined in Source 2.  The provided text does not offer enough detail to definitively classify your tool.\\n\\n**Strategic Recommendation:**\\n\\nTo ensure compliance and avoid potential regulatory issues, I strongly recommend a thorough review of the FDA's guidance on Software as a Medical Device (SaMD).  A detailed analysis of your tool's functionality and intended use is crucial to determine the appropriate regulatory pathway and necessary documentation.  Consulting with regulatory experts specializing in medical device software is also highly advisable.  This proactive approach will mitigate risks and ensure your product's successful market entry.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate agent objects and run\n",
    "\n",
    "ragagent = RAGAgent()\n",
    "response_agent = ResponseAgent()\n",
    "orchestrator = OrchestratorAgent([ragagent, response_agent])\n",
    "\n",
    "orchestrator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b8cf0-ec29-4a8c-b79d-a3c4ae9a7d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
