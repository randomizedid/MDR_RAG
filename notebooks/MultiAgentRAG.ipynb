{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb86a40c-00bd-4dfa-bd28-67dce3e720b1",
   "metadata": {},
   "source": [
    "This Jupyter Notebook serves as a proof of concept of a multi-agent solution for MedTech regulations. The intent of the system is to provide clear answers to questions on WHO and FDA documentations on medical devices, using a 3-to-4 agents system composed like this:\n",
    "\n",
    "- an LLM orchestrator that receives the question and coordinates agents\n",
    "- a RAG agent capable of retrieving documents related to the question\n",
    "- an LLM response agent to put together the answer based on the documents retrieved and the question\n",
    "- a possible fourth agent to be decided (summary agent, source-verifier, compare agent, prompt agent to improve prompts, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae99977-325e-4a78-b556-a63ccb3dca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\npseudo code for multi agent system\\n\\nclass agent\\n\\ninstantiate orchestrator and other agents\\n\\ndefine orchestrator prompt and response false\\n\\ntake input\\n\\nwhile not response:\\n\\n    orchestrator call\\n\\n    designed agent call\\n\\nreturn response\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "pseudo code for multi agent system\n",
    "\n",
    "class agent\n",
    "\n",
    "instantiate orchestrator and other agents\n",
    "\n",
    "define orchestrator prompt and response false\n",
    "\n",
    "take input\n",
    "\n",
    "while not response:\n",
    "\n",
    "    orchestrator call\n",
    "\n",
    "    designed agent call\n",
    "\n",
    "return response\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40713cc2-97db-4577-a884-fd2fb1f081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "import uuid\n",
    "from pypdf import PdfReader\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import pymupdf4llm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30825fc5-b1f5-43de-a6e7-c9f0ce7f3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environmental variables and AI models\n",
    "\n",
    "load_dotenv() #loads the API key put in a .env file\n",
    "try:\n",
    "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Google AI. Please ensure your API key is correct. Error: {e}\")\n",
    "\n",
    "# Configure paths to data\n",
    "current_dir = os.getcwd() \n",
    "FILE_PATH_FDA_DESIGN = os.path.join(current_dir, '..', 'documents', 'FDA_Design_Control_Guidance.pdf')\n",
    "FILE_PATH_WHO = os.path.join(current_dir, '..', 'documents', 'WHO_Medical_Device_Regulations.pdf')\n",
    "FILE_PATH_FDA_POLICY = os.path.join(current_dir, '..', 'documents', 'FDA_Policy_Device_Software_Functions.pdf')\n",
    "COLLECTION_NAME = \"multi_agent_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6997c9-03e2-4a41-8f6a-327a00857e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657 1657\n"
     ]
    }
   ],
   "source": [
    "# Split logic (very simple, splitting paragraphs)\n",
    "\n",
    "def split_into_chunks(dict_list):\n",
    "    text_chunks = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for document in dict_list:\n",
    "        try:\n",
    "            paragraphs = list(document.values())[0].split(\"\\n\\n\")\n",
    "    \n",
    "            for paragraph in paragraphs: # Delete short paragraphs\n",
    "                if len(paragraph) > 10:\n",
    "                    text_chunks.append(paragraph)\n",
    "                    metadatas.append({\"source\": list(document.keys())[0]})\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {list(document.keys())[0]}: {e}\")\n",
    "            \n",
    "    return text_chunks, metadatas\n",
    "\n",
    "# Use pymupdf4llm to convert pdf into text, adequately formatted. Using dicts to keep track of sources and add them to metadata while chunking\n",
    "fda_design_dict = {'FDA_Design_Control_Guidance.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_DESIGN)}\n",
    "who_dict = {'WHO_Medical_Device_Regulations.pdf': pymupdf4llm.to_markdown(FILE_PATH_WHO)}\n",
    "fda_policy_dict = {'FDA_Policy_Device_Software_Functions.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_POLICY)}\n",
    "\n",
    "text_chunks, metadatas = split_into_chunks([fda_design_dict, who_dict, fda_policy_dict])\n",
    "print(len(text_chunks), len(metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f1732-06ae-4dca-8b80-138559008a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "455b173f-006d-4f4a-8de5-e754aef4524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent classes\n",
    "\n",
    "class OrchestratorAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.prompt = \"\"\"\n",
    "        Act as an orchestrator agent for an intelligent RAG system for MedTech companies. Your task is to coordinate agents in order to extract relevant documents from a RAG system and package a coherent and precise answer to the query received.\n",
    "        The task is to call a first time the ragagent, and then use the ragagent documents to call the response_agent on them and craft a response. Make sure to call the response_agent if the history contains a previous call to the ragagent.\n",
    "        Your AI agents are: \n",
    "        - ragagent: the agent responsible for querying the RAG system and returning relevant documents and scores, based on the query. Expects a query as input and outputs documents, metadatas and sources. \n",
    "        - response_agent: the agent who will craft the response based on the query and the relevant documents provided by the ragagent. Expects a query and documents as inputs and outputs a string.\n",
    "\n",
    "        You will return instructions in a JSON format. For the ragagent will be like:\n",
    "        {\n",
    "            \"agent_name\": \"ragagent\",\n",
    "            \"query\": String\n",
    "            \"notes\": String\n",
    "        }\n",
    "        while for the response agent:\n",
    "        {\n",
    "            \"agent_name\": \"response_agent,\n",
    "            \"query\": String\n",
    "            \"relevant documents\": [chunk_dict1, chunk_dict2, chunk_dict3]\n",
    "            \"relevant_sources\": [source1, source2, source3]\n",
    "            \"notes\": String\n",
    "        }\n",
    "        At every step, you need to choose only one of the agents based on the history you will be provided (es. blank history will default to rag, while a history with a past call to rag will go to a response) and provide instruction to onyl that agent.\n",
    "        where chunk dicts will contain the text document as key and the distance score as value. You can use the notes parameter to add consideration to the system, such as way to improve it or which agent you think it would be better to call at the current step (it can also be an agent that is not in your pool but you think would be useful).\n",
    "        Attached to this prompt, you will find additional information on the history of agent calls and their outputs, to provide context for the next call.\n",
    "        \"\"\"\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "    def call_agent(self, history, information):\n",
    "        prompt = self.prompt + \"\\n\\n\" + \" \".join(json.dumps(history)) + \"\\n\\n\" + information\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response\n",
    "\n",
    "class RAGAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        from chromadb.utils import embedding_functions\n",
    "        self.sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.collection_name = COLLECTION_NAME\n",
    "\n",
    "    def act(self, history, query):\n",
    "        if \"RAG initialized\" not in history:\n",
    "            self.collection = self.initialize_db()\n",
    "            history.append(\"RAG initialized\")\n",
    "    \n",
    "        return self.query_db(query)\n",
    "\n",
    "    def initialize_db(self):\n",
    "        print(\"Initializing RAG system... This may take a minute.\")\n",
    "        self.client = chromadb.Client()\n",
    "\n",
    "        if self.collection_name in [c.name for c in self.client.list_collections()]:\n",
    "            self.client.delete_collection(name = self.collection_name)\n",
    "            print(f\"Deleted existing collection: {self.collection_name}\"\n",
    "                 )\n",
    "    \n",
    "        collection = self.client.get_or_create_collection(\n",
    "        name = self.collection_name,\n",
    "        embedding_function = self.sentence_transformer_ef\n",
    "        )\n",
    "        self.load_documents(collection, text_chunks, metadatas)\n",
    "\n",
    "        return collection\n",
    "\n",
    "    def load_documents(self, collection, document_chunks, metadatas):\n",
    "        collection.add(\n",
    "        ids = [str(uuid.uuid4()) for _ in text_chunks],\n",
    "        documents = text_chunks,\n",
    "        metadatas = metadatas)\n",
    "    \n",
    "    def query_db(self, question):\n",
    "        results = self.collection.query(query_texts=[question], include = [\"documents\", \"metadatas\", \"distances\"], n_results=5)\n",
    "\n",
    "        sources_markdown = \"### Sources Used for Analysis\\n\\n\"\n",
    "        retrieved_documents = results['documents'][0]\n",
    "        retrieved_metadatas = results['metadatas'][0]\n",
    "        retrieved_distances = results['distances'][0]\n",
    "    \n",
    "        for i, (doc, meta, dist) in enumerate(zip(retrieved_documents, retrieved_metadatas, retrieved_distances)):\n",
    "            # Convert distance to a more intuitive similarity score (1 - distance)\n",
    "            relevance_score = 1 - dist\n",
    "            source_info = f\"**Source {i+1}:** {meta.get('source', 'N/A')}, Page {meta.get('page', 'N/A')}\\n\"\n",
    "            relevance_info = f\"**Relevance Score:** {relevance_score:.2f}\\n\\n\"\n",
    "            content_info = f\"```\\n{doc}\\n```\\n\\n---\\n\\n\"\n",
    "            sources_markdown += source_info + relevance_info + content_info\n",
    "            \n",
    "        return retrieved_documents, sources_markdown\n",
    "\n",
    "class ResponseAgent:\n",
    "    def __init__(self):\n",
    "        self.prompt = \"\"\"\n",
    "        Act as a Senior Consultant for medical devices. You receive a query from your client, and answer to it based on the relevant information you receive from the RAG system as document chunks.\n",
    "        Be precise and do not make things up. If the context is not enough to provide a clear answer, state it.\n",
    "        Cite the documents and sources you receive as part of your input and provide strategic recommendation. The structure of your answer will be:\n",
    "        - Salutation\n",
    "        - Precise response to the query based on the documents received from the RAG\n",
    "        - Strategic recommendation to the customer.\n",
    "        Attached to this prompt, you will find a history containing the query, past responses from the system and the relevant document for the analysis and response.\n",
    "        \"\"\"\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    def craft_response(self, query, history):\n",
    "        prompt = self.prompt + \"\\n\\n\" + query + \"\\n\\n\" + \" \".join(json.dumps(history))\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e34d4077-9071-4fb7-b4c6-2d7c39f0bc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"Documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"Focus on regulatory compliance (e.g., HIPAA, GDPR, FDA),  security, privacy, and user interface/usability aspects of the documentation.  Next step should be to the response agent.\"\n",
      "}\n",
      "```\n",
      "\n",
      "8 319\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "Initializing RAG system... This may take a minute.\n",
      "Deleted existing collection: multi_agent_rag\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"Documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"Focus on regulatory, privacy, and user interface documentation.  Next step should be to the response_agent to synthesize a concise answer.\"\n",
      "}\n",
      "```\n",
      "\n",
      "8 278\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"Documentation requirements for a mobile heart rate monitoring application.\",\n",
      "  \"notes\": \"Focus on regulatory requirements (e.g., FDA, HIPAA, GDPR),  security considerations, and user privacy aspects.  Prioritize documents related to software design, testing, and validation. After this, the response agent should be called to synthesize the findings.\"\n",
      "}\n",
      "```\n",
      "\n",
      "8 402\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count_rag \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m count_final \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     38\u001b[0m         final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinito\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_answer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Instantiate agent objects\n",
    "\n",
    "def clean_response(response):\n",
    "    response_text = response.text\n",
    "    print(response_text)\n",
    "    start_index = response_text.find(\"{\")\n",
    "    end_index = response_text.find(\"}\")\n",
    "    print(start_index, end_index)\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return json.loads(response_text[start_index : end_index + 1])\n",
    "    else:\n",
    "        return response_text\n",
    "\n",
    "orchestrator = OrchestratorAgent()\n",
    "ragagent = RAGAgent()\n",
    "response_agent = ResponseAgent()\n",
    "\n",
    "final_answer = None\n",
    "history = []\n",
    "query = \"What documentation is needed for a mobile app that monitors heart rate?\"\n",
    "count_rag = 0\n",
    "count_final = 0\n",
    "\n",
    "while not final_answer:\n",
    "    response_json = clean_response(orchestrator.call_agent(history, query))\n",
    "    history.append(response_text)\n",
    "    print(response_text)\n",
    "    if json_response[\"agent_name\"] == \"ragagent\":\n",
    "        count_rag += 1\n",
    "        #retrieved_documents, sources_markdown = ragagent.act(history, query)\n",
    "        history.extend(ragagent.act(history, query))\n",
    "    elif json_response[\"agent_name\"] == \"response_agent\":\n",
    "        count_final += 1\n",
    "        final_answer = response_agent.craft_response(query, history)\n",
    "    else:\n",
    "        print(\"there is an error, the agent called does not exist\")\n",
    "    if count_rag == 3 or count_final == 3:\n",
    "        final_answer = \"Finito\"\n",
    "\n",
    "print(final_answer.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde820ce-0e23-4632-bba8-aa08b63e92bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263bcd6-d7e8-4566-9b6a-578686f4df6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89678ba-472f-4dea-92b8-f0378fcf9f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ea1f8-4a2b-4255-bcac-e5b23e55bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
