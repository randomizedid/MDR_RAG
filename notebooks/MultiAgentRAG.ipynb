{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb86a40c-00bd-4dfa-bd28-67dce3e720b1",
   "metadata": {},
   "source": [
    "This Jupyter Notebook serves as a proof of concept of a multi-agent solution for MedTech regulations. The intent of the system is to provide clear answers to questions on WHO and FDA documentations on medical devices, using a 3-to-4 agents system composed like this:\n",
    "\n",
    "- an LLM orchestrator that receives the question and coordinates agents\n",
    "- a RAG agent capable of retrieving documents related to the question\n",
    "- an LLM response agent to put together the answer based on the documents retrieved and the question\n",
    "- a possible fourth agent to be decided (summary agent, source-verifier, compare agent, prompt agent to improve prompts, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae99977-325e-4a78-b556-a63ccb3dca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\npseudo code for multi agent system\\n\\nclass agent\\n\\ninstantiate orchestrator and other agents\\n\\ndefine orchestrator prompt and response false\\n\\ntake input\\n\\nwhile not response:\\n\\n    orchestrator call\\n\\n    designed agent call\\n\\nreturn response\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "pseudo code for multi agent system\n",
    "\n",
    "class agent\n",
    "\n",
    "instantiate orchestrator and other agents\n",
    "\n",
    "define orchestrator prompt and response false\n",
    "\n",
    "take input\n",
    "\n",
    "while not response:\n",
    "\n",
    "    orchestrator call\n",
    "\n",
    "    designed agent call\n",
    "\n",
    "return response\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40713cc2-97db-4577-a884-fd2fb1f081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "import uuid\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pymupdf4llm\n",
    "import json\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30825fc5-b1f5-43de-a6e7-c9f0ce7f3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environmental variables and AI models\n",
    "\n",
    "load_dotenv() #loads the API key put in a .env file\n",
    "try:\n",
    "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Google AI. Please ensure your API key is correct. Error: {e}\")\n",
    "\n",
    "# Configure paths to data\n",
    "current_dir = os.getcwd() \n",
    "FILE_PATH_FDA_DESIGN = os.path.join(current_dir, '..', 'documents', 'FDA_Design_Control_Guidance.pdf')\n",
    "FILE_PATH_WHO = os.path.join(current_dir, '..', 'documents', 'WHO_Medical_Device_Regulations.pdf')\n",
    "FILE_PATH_FDA_POLICY = os.path.join(current_dir, '..', 'documents', 'FDA_Policy_Device_Software_Functions.pdf')\n",
    "COLLECTION_NAME = \"multi_agent_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6997c9-03e2-4a41-8f6a-327a00857e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657 1657\n"
     ]
    }
   ],
   "source": [
    "# Split logic (very simple, splitting paragraphs)\n",
    "\n",
    "def split_into_chunks(dict_list):\n",
    "    text_chunks = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for document in dict_list:\n",
    "        try:\n",
    "            paragraphs = list(document.values())[0].split(\"\\n\\n\")\n",
    "    \n",
    "            for paragraph in paragraphs: # Delete short paragraphs\n",
    "                if len(paragraph) > 10:\n",
    "                    text_chunks.append(paragraph)\n",
    "                    metadatas.append({\"source\": list(document.keys())[0]})\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {list(document.keys())[0]}: {e}\")\n",
    "            \n",
    "    return text_chunks, metadatas\n",
    "\n",
    "# Use pymupdf4llm to convert pdf into text, adequately formatted. Using dicts to keep track of sources and add them to metadata while chunking\n",
    "fda_design_dict = {'FDA_Design_Control_Guidance.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_DESIGN)}\n",
    "who_dict = {'WHO_Medical_Device_Regulations.pdf': pymupdf4llm.to_markdown(FILE_PATH_WHO)}\n",
    "fda_policy_dict = {'FDA_Policy_Device_Software_Functions.pdf': pymupdf4llm.to_markdown(FILE_PATH_FDA_POLICY)}\n",
    "\n",
    "text_chunks, metadatas = split_into_chunks([fda_design_dict, who_dict, fda_policy_dict])\n",
    "print(len(text_chunks), len(metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e7e25a2f-373c-4d86-917d-19c23ee619f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed FDA_Design: 67 chunks.\n",
      "Processed WHO: 120 chunks.\n",
      "Processed FDA Policy: 100 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Second chunking logic: texting up the documents. I'll convert the pdf in txt and see if the RAG handles them better.\n",
    "\n",
    "# Configure paths to data\n",
    "current_dir = os.getcwd() \n",
    "FILE_PATH_FDA_DESIGN = os.path.join(current_dir, '..', 'documents', 'FDA_Design_Control_Guidance.txt')\n",
    "FILE_PATH_WHO = os.path.join(current_dir, '..', 'documents', 'WHO_Medical_Device_Regulations.txt')\n",
    "FILE_PATH_FDA_POLICY = os.path.join(current_dir, '..', 'documents', 'FDA_Policy_Device_Software_Functions.txt')\n",
    "COLLECTION_NAME = \"multi_agent_rag\"\n",
    "\n",
    "def process_txt_file(file_path, source_name):\n",
    "    \"\"\"\n",
    "    Reads a .txt file and splits it into chunks based on paragraphs\n",
    "    (separated by double newlines).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .txt file.\n",
    "        source_name (str): The name of the source document for metadata.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists: (text_chunks, metadatas).\n",
    "    \"\"\"\n",
    "    all_text_chunks = []\n",
    "    all_metadatas = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        # Chunking strategy: split by double newline (a common paragraph separator)\n",
    "        paragraphs = full_text.split('\\n\\n')\n",
    "\n",
    "        for para in paragraphs:\n",
    "            stripped_para = para.strip()\n",
    "            if len(stripped_para) > 25:  # Filter out very short or empty lines\n",
    "                all_text_chunks.append(stripped_para)\n",
    "                # Metadata is simpler for a txt file, just the source\n",
    "                all_metadatas.append({'source': source_name})\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "    return all_text_chunks, all_metadatas\n",
    "\n",
    "chunks_fdad, metas_fdad = process_txt_file(FILE_PATH_FDA_DESIGN, 'FDA_Design_Control_Guidance')\n",
    "print(f\"Processed FDA_Design: {len(chunks_fdad)} chunks.\")\n",
    "\n",
    "chunks_who, metas_who = process_txt_file(FILE_PATH_WHO, 'WHO_Medical_Device_Regulations')\n",
    "print(f\"Processed WHO: {len(chunks_who)} chunks.\")\n",
    "\n",
    "chunks_fdap, metas_fdap = process_txt_file(FILE_PATH_FDA_POLICY, 'FDA_Policy_Device_Software_Functions')\n",
    "print(f\"Processed FDA Policy: {len(chunks_fdap)} chunks.\")\n",
    "\n",
    "text_chunks = chunks_fdad + chunks_who + chunks_fdapS\n",
    "metadatas = metas_fdad + metas_who + metas_fdap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ed028aee-0d9f-4fce-8908-9151bc8a89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Center for Devices and Radiological Health\n",
      "DESIGN CONTROL GUIDANCE\n",
      "FOR\n",
      "MEDICAL DEVICE MANUFACTURERS\n",
      "This Guidance relates to\n",
      "FDA 21 CFR 820.30 and Sub-clause 4.4 of ISO 9001'\n",
      "page_content='FOREWORD\n",
      "To ensure that good quality assurance practices are used for the design of medical devices\n",
      "and that they are consistent with quality system requirements worldwide, the Food and'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2093"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Langchain recursive tex splitter NEED TO ADD METADATAS THOUGH\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "file_paths = [FILE_PATH_FDA_DESIGN, FILE_PATH_WHO, FILE_PATH_FDA_POLICY]\n",
    "\n",
    "all_text = \"\"\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_text += f.read() + \"\\n\\n\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.create_documents([all_text])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f1732-06ae-4dca-8b80-138559008a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "455b173f-006d-4f4a-8de5-e754aef4524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent classes\n",
    "\n",
    "class OrchestratorAgent:\n",
    "\n",
    "    def __init__(self, agents):\n",
    "        self.agents = agents\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Hi! I am your MedTech regulatory companion. How can I help you?\")\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        while True:\n",
    "            self.memory = self.memory[-self.memory_limit:]\n",
    "            if user_input.lower() in [\"exit\", \"bye\", \"close\"]:\n",
    "                print(\"I hope I could be of use to you, have a great day!\")\n",
    "                break\n",
    "                \n",
    "            orch_response = self.orchestrate(user_input)\n",
    "            if orch_response[\"agent_to_call\"] == \"No action needed\":\n",
    "                print(\"Is there anything else I can help you with?\")\n",
    "                user_input = input(\"You: \")\n",
    "            for agent in self.agents:\n",
    "                if agent.name == orch_response[\"agent_to_call\"]:\n",
    "                    print(f\"Found agent I was looking for: {agent.name}\\n\")\n",
    "                    response = agent.act(orch_response[\"output\"], orch_response[\"relevant_info\"], self.memory)\n",
    "                    self.memory.append(f\"Agent {agent.name} responded {response}\")\n",
    "                    #controllo da togliere\n",
    "                    if agent.name == \"response_agent\":\n",
    "                        return\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def orchestrate(self, user_input):\n",
    "        self.memory.append(f\"User: {user_input}\")\n",
    "        self.memory = self.memory[-self.memory_limit:]\n",
    "        context = \"\\n\".join(self.memory)\n",
    "\n",
    "        response_format = {\"agent_to_call\":\"\", \"output\": \"\", \"relevant_info\":\"\"}\n",
    "        response = self.model.generate_content(self.get_prompt(context, response_format))\n",
    "        self.memory.append(f\"Orchestrator: {response.text}\")\n",
    "        response_cleaned = clean_response(response)\n",
    "        #print(f\"The next agent to call is {response_cleaned['agent_to_call']}\")\n",
    "        return response_cleaned\n",
    "\n",
    "    def get_prompt(self, context, response_format):\n",
    "        prompt = f\"\"\"\n",
    "        Act as an orchestrator agent for an intelligent RAG system for MedTech companies. Your task is to coordinate agents in order to extract relevant documents from a RAG system and package a coherent and precise answer to the query received.\n",
    "        The task is to call a first time the ragagent, and then use the ragagent documents to call the response_agent on them and craft a response. Make sure to call the response_agent if the history contains a previous call to the ragagent.\n",
    "        Your AI agents and their descriptions are {\", \".join([f\"- {agent.name}: {agent.description}\" for agent in self.agents])}\n",
    "\n",
    "        Use the context, which includes the current user input and the memory of previous inputs and outputs, to plan next steps.\n",
    "        Context : {context}\n",
    "\n",
    "        Guidelines:\n",
    "        At every step, you need to choose only one of the agents provide instruction to only that agent. If the request needs multiple agent to be solved, do that in a loop.\n",
    "        Read the context, take your time to understand the task, and check if you have executed it correctly.\n",
    "        If there are no actions needed, default the \"agent_to_call\" parameter to \"No action needed\" in the response.\n",
    "        Return only the agent name in the \"agent_to_call\" parameter.\n",
    "        You will return instructions in a valid JSON in the form of {response_format}. All output should be of string type, the \"output\" is for the query and the \"relevant_info\" is to attach documents from the RAG for the response agent.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "        \n",
    "    def call_agent(self, history, information):\n",
    "        prompt = self.prompt + \"\\n\\n\" + \" \".join(json.dumps(history)) + \"\\n\\n\" + information\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response\n",
    "\n",
    "class RAGAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"ragagent\"\n",
    "        self.description = \"\"\"I am a RAG agent that can search for relevant documents in a vector database in order to answer a query.\n",
    "                            I expect a user query as input and will return relevant chunks and a variable containing sources info, relevance scores and chunks.\n",
    "                            \"\"\"\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "        self.sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.collection_name = COLLECTION_NAME\n",
    "\n",
    "    def act(self, query, relevant_info, memory):\n",
    "        if \"RAG initialized\" not in history:\n",
    "            self.collection = self.initialize_db()\n",
    "            self.memory.append(\"RAG initialized\")\n",
    "        self.memory.append(memory)\n",
    "        self.memory = self.memory[-self.memory_limit:]\n",
    "        documents = self.query_db(query)\n",
    "        return documents\n",
    "\n",
    "    def initialize_db(self):\n",
    "        print(\"Initializing RAG system... This may take a minute.\")\n",
    "        self.client = chromadb.Client()\n",
    "\n",
    "        if self.collection_name in [c.name for c in self.client.list_collections()]:\n",
    "            self.client.delete_collection(name = self.collection_name)\n",
    "            print(f\"Deleted existing collection: {self.collection_name}\"\n",
    "                 )\n",
    "    \n",
    "        collection = self.client.get_or_create_collection(\n",
    "        name = self.collection_name,\n",
    "        embedding_function = self.sentence_transformer_ef\n",
    "        )\n",
    "        self.load_documents(collection, text_chunks, metadatas)\n",
    "\n",
    "        return collection\n",
    "\n",
    "    def load_documents(self, collection, document_chunks, metadatas):\n",
    "        collection.add(\n",
    "        ids = [str(uuid.uuid4()) for _ in text_chunks],\n",
    "        documents = text_chunks,\n",
    "        metadatas = metadatas)\n",
    "    \n",
    "    def query_db(self, question):\n",
    "        results = self.collection.query(query_texts=[question], include = [\"documents\", \"metadatas\", \"distances\"], n_results=5)\n",
    "\n",
    "        sources_markdown = \"### Sources Used for Analysis\\n\\n\"\n",
    "        retrieved_documents = results['documents'][0]\n",
    "        retrieved_metadatas = results['metadatas'][0]\n",
    "        retrieved_distances = results['distances'][0]\n",
    "    \n",
    "        for i, (doc, meta, dist) in enumerate(zip(retrieved_documents, retrieved_metadatas, retrieved_distances)):\n",
    "            # Convert distance to a more intuitive similarity score (1 - distance)\n",
    "            relevance_score = 1 - dist\n",
    "            source_info = f\"**Source {i+1}:** {meta.get('source', 'N/A')}, Page {meta.get('page', 'N/A')}\\n\"\n",
    "            relevance_info = f\"**Relevance Score:** {relevance_score:.2f}\\n\\n\"\n",
    "            content_info = f\"```\\n{doc}\\n```\\n\\n---\\n\\n\"\n",
    "            sources_markdown += source_info + relevance_info + content_info\n",
    "        print(f\"Sources, documents and relevance score: {sources_markdown}\")\n",
    "            \n",
    "        return sources_markdown\n",
    "\n",
    "class ResponseAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"response_agent\"\n",
    "        self.description = \"\"\"I am a response agent that expects as input a user query and relevant documents and info from a RAG search.\n",
    "                            My task is to craft a precise response for the user based on the provided documents. I will return a response text.\n",
    "                            \"\"\"\n",
    "\n",
    "        self.memory = []\n",
    "        self.memory_limit = 15\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    def act(self, query, relevant_info, memory):\n",
    "        prompt = self.get_prompt(query, relevant_info, memory)\n",
    "        response = self.model.generate_content(prompt)\n",
    "        print(response.text)\n",
    "        return response.text\n",
    "\n",
    "    def get_prompt(self, query, relevant_info, memory):\n",
    "        prompt = f\"\"\"\n",
    "        Act as a Senior Consultant for medical devices. You receive a query from your client, and answer to it based on the relevant information you receive from the RAG system as document chunks.\n",
    "        Be precise, confident and do not make things up. If the context is not enough to provide a clear answer, state it.\n",
    "        Cite the documents and sources you receive as part of your input and provide strategic recommendation. The structure of your answer will be:\n",
    "        - Salutation\n",
    "        - Precise response to the query based on the documents received from the RAG\n",
    "        - Strategic recommendation to the customer.\n",
    "\n",
    "        As additional resources:\n",
    "        User input: {query}\n",
    "        Relevant documents with sources and relevance scores: {relevant_info}\n",
    "        Memory of previous inputs and info: {memory}\n",
    "        \"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331d425-f4dd-4400-b1e7-d1badb209985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why not add an agent to do chunking? add pydantic to force the format of the answer\n",
    "# it would make more sense to generalize the response_agent class since it is an llm calling, let's see if we have time\n",
    "\n",
    "class ChunkingAgent():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e34d4077-9071-4fb7-b4c6-2d7c39f0bc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"Documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"Focus on regulatory compliance (e.g., HIPAA, GDPR, FDA),  security, privacy, and user interface/usability aspects of the documentation.  Next step should be to the response agent.\"\n",
      "}\n",
      "```\n",
      "\n",
      "8 319\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "Initializing RAG system... This may take a minute.\n",
      "Deleted existing collection: multi_agent_rag\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"Documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"Focus on regulatory, privacy, and user interface documentation.  Next step should be to the response_agent to synthesize a concise answer.\"\n",
      "}\n",
      "```\n",
      "\n",
      "8 278\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"Documentation requirements for a mobile heart rate monitoring application.\",\n",
      "  \"notes\": \"Focus on regulatory requirements (e.g., FDA, HIPAA, GDPR),  security considerations, and user privacy aspects.  Prioritize documents related to software design, testing, and validation. After this, the response agent should be called to synthesize the findings.\"\n",
      "}\n",
      "```\n",
      "\n",
      "8 402\n",
      "```json\n",
      "{\n",
      "  \"agent_name\": \"ragagent\",\n",
      "  \"query\": \"documentation requirements for a mobile heart rate monitoring application\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count_rag \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m count_final \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     38\u001b[0m         final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinito\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_answer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Instantiate agent objects\n",
    "\n",
    "def clean_response(response):\n",
    "    response_text = response.text\n",
    "    print(response.text)\n",
    "    start_index = response_text.find(\"{\")\n",
    "    end_index = response_text.rfind(\"}\")\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return json.loads(response_text[start_index : end_index + 1])\n",
    "    else:\n",
    "        return response_text\n",
    "\n",
    "orchestrator = OrchestratorAgent()\n",
    "ragagent = RAGAgent()\n",
    "response_agent = ResponseAgent()\n",
    "\n",
    "final_answer = None\n",
    "history = []\n",
    "query = \"What documentation is needed for a mobile app that monitors heart rate?\"\n",
    "count_rag = 0\n",
    "count_final = 0\n",
    "\n",
    "while not final_answer:\n",
    "    response_json = clean_response(orchestrator.call_agent(history, query))\n",
    "    history.append(response_text)\n",
    "    print(response_text)\n",
    "    if json_response[\"agent_name\"] == \"ragagent\":\n",
    "        count_rag += 1\n",
    "        #retrieved_documents, sources_markdown = ragagent.act(history, query)\n",
    "        history.extend(ragagent.act(history, query))\n",
    "    elif json_response[\"agent_name\"] == \"response_agent\":\n",
    "        count_final += 1\n",
    "        final_answer = response_agent.craft_response(query, history)\n",
    "    else:\n",
    "        print(\"there is an error, the agent called does not exist\")\n",
    "    if count_rag == 3 or count_final == 3:\n",
    "        final_answer = \"Finito\"\n",
    "\n",
    "print(final_answer.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde820ce-0e23-4632-bba8-aa08b63e92bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263bcd6-d7e8-4566-9b6a-578686f4df6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89678ba-472f-4dea-92b8-f0378fcf9f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ea1f8-4a2b-4255-bcac-e5b23e55bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
